{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run boilerplate code to set up environment\n",
    "\n",
    "%run ../prelude.py\n",
    "\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triangular solver\n",
    "\n",
    "Based on scipy's [scipy.linalg.solve_triangular](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve_triangular.html) and the solver code presented by [Weng et al. at HPCA'20](https://ieeexplore.ieee.org/abstract/document/9065593)\n",
    "\n",
    "Consider the following system of equations:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_{11} x_1&              &= b_1 \\\\\n",
    "a_{21} x_1&+ a_{22} x_2 &= b_2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This can be expressed as a matrix equation $\\mathbf{A}x=b$ where\n",
    "$\n",
    "\\mathbf{A} = \\left[\\begin{array}{c c}\n",
    "  a_{11} & 0      \\\\\n",
    "  a_{21} & a_{22}\n",
    "  \\end{array}\n",
    "  \\right]$\n",
    "and $\n",
    "b = \\left[\\begin{array}{c}\n",
    "  b_1 \\\\ b_2 \\end{array} \\right]\n",
    "$.\n",
    "\n",
    "To solve, you would conclude that $x_1 = b_1 / a_{11}$, and plug it into the second equation to solve for $x_2$. However, this would correspond to a column-major traversal of $\\mathbf{A}$, so we transpose $\\mathbf{A}$ to get a row-major (and hopefully concordant) traversal.\n",
    "\n",
    "(Evidence of this transposition manifests below as generating `A_data` as an _upper_ triangular matrix, as well as the `trans=\"T\"` keyword argument passed to `solve_triangular()` in the assertion.)\n",
    "\n",
    "_Note:_ this was developed expecting uncompressed tensors but its use of `getPayload{,Ref}()` should support compressed formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = N = 4\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Generate a random upper triangular matrix A\n",
    "A_data = np.triu(np.random.randint(1, 6, size=(M, N)))\n",
    "\n",
    "# Generate the desired output vector x (Ax=b)\n",
    "X_ref_data = np.array(np.random.randint(1, 6, size=(N,)))\n",
    "\n",
    "# Evaluate the triangular matrix with X_ref to obtain B\n",
    "#   this corresponds to a column-wise dot product with \n",
    "B_data = X_ref_data.dot(A_data)\n",
    "\n",
    "print(\"A:\\n\", A_data, \"\\nx (reference):\\n\", X_ref_data, \"\\nb:\\n\", B_data)\n",
    "\n",
    "assert(all(X_ref_data == scipy.linalg.solve_triangular(A_data, B_data, trans=\"T\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create uncompressed tensors\n",
    "A_MN = Tensor.fromUncompressed([\"M\", \"N\"], A_data.tolist()).setName(\"A\")\n",
    "B_N = Tensor.fromUncompressed([\"N\"], B_data.tolist()).setName(\"b\")\n",
    "X_ref = Tensor.fromUncompressed([\"M\"], X_ref_data.tolist()).setName(\"x_ref\")\n",
    "X = Tensor(rank_ids=[\"M\"]).setName(\"x\")\n",
    "\n",
    "A = A_MN.getRoot()\n",
    "\n",
    "inflate = False\n",
    "if inflate:\n",
    "    # hack to \"inflate\" A with coordinates where there are zeros,\n",
    "    # and allowing us to use square brackets (i.e. __getitem__)\n",
    "    # because positions == coordinates\n",
    "    A << Fiber(coords=range(M))\n",
    "    for j, a_j in A:\n",
    "        a_j << Fiber(coords=range(N))\n",
    "\n",
    "B = B_N.getRoot()\n",
    "\n",
    "displayTensor(A)\n",
    "displayTensor(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelined triangular solver\n",
    "\n",
    "`P` is a representation of pipelined communication; each row corresponds to the data passed from the previous stage to the next. Each row is N elements wide (same as B).\n",
    "\n",
    "This mapping corresponds to a pipeline of single-threaded compute engines (CEs) commensurate in size to the input.\n",
    "\n",
    "_Note:_ I traverse A in the N rank, then the M rank. I think I need a `swapRanks()` somewhere..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create \"pipes\" between each stage: N-entry tensors\n",
    "# (can't simply do a rank-2 tensor because I'd have multiple entries for addActivity for that tensor, and\n",
    "# current pipelining API doesn't really handle that\n",
    "#\n",
    "# for syntactic cleanliness I include B, the input tensor, in this array too\n",
    "BP = [B_N] + [Tensor(rank_ids=[\"N\"], shape=[N]).setName(f\"pipe-{m}\") for m in range(M)]\n",
    "\n",
    "canvas = createCanvas(A_MN, *BP, X, enable_wait=True)\n",
    "\n",
    "cycle = 0\n",
    "stage_delay = 1\n",
    "\n",
    "# \"wait\" means, for each key-value pair in the supplied dictionary,\n",
    "# we must wait `value` cycles when the `key`th argument in the canvas has been updated \n",
    "# in order to add our own activity\n",
    "wait = None # loop-carried variable\n",
    "\n",
    "for a_m in range(M):\n",
    "    # in the first row, read from B, rather than P\n",
    "    # also, skew these starts by cycle, as these input elements are piped in one after another\n",
    "    # m_ and n_activities provide pair-wise activity locations for adjacent stages in the pipeline\n",
    "    # e.g., [(m,), (m,), ()] then [(), (m,), (m,)] then [(), (), (m,)]\n",
    "    m_activities = [(a_m,) if mm == a_m or mm == a_m+1 else () for mm in range(M+1)]\n",
    "    # print(m_activities)\n",
    "\n",
    "    # process element on diagonal\n",
    "    curStage = BP[a_m]\n",
    "    nextStage = BP[a_m+1]\n",
    "    a_n = a_m\n",
    "    next_m = nextStage.getPayloadRef(a_m)\n",
    "    next_m <<= curStage.getPayload(a_m) / A.getPayload(a_m, a_n)\n",
    "    addActivity(canvas, (a_m, a_n), *m_activities, worker=str(a_m), skew=(cycle if a_m == 0 else 0), wait=wait)\n",
    "    cycle += 1\n",
    "    \n",
    "    # process elements off-diagonal on same row\n",
    "    # change from a_m to a_n (even though M=N) for clarity\n",
    "    for i_n in range(a_n+1, N):\n",
    "        n_activities = [(i_n,) if ii == a_n or ii == a_n+1 else () for ii in range(N+1)]\n",
    "        # print(n_activities)\n",
    "        \n",
    "        cur_n = curStage.getPayload(i_n)\n",
    "        next_n = nextStage.getPayloadRef(i_n)\n",
    "        next_n <<= cur_n - next_m * A.getPayload(a_m, i_n)\n",
    "\n",
    "        addActivity(canvas, (a_m, i_n), *n_activities, worker=str(a_m), skew=(cycle if a_m == 0 else 0), wait=wait)\n",
    "        cycle += 1\n",
    "        \n",
    "    # new values for next value of a_m\n",
    "    wait = {f\"pipe-{a_m}\": stage_delay}\n",
    "\n",
    "for x_m in range(M):\n",
    "    x_m_ref = X.getPayloadRef(x_m)\n",
    "    x_m_ref <<= BP[x_m+1].getPayload(x_m) # get m-th value from pipeline stage m\n",
    "    m_activities = [(x_m,) if mm == x_m else () for mm in range(M)]\n",
    "    # print(x_m, [], [], m_activities + [(x_m,)])\n",
    "    addActivity(canvas, [], [], *m_activities, (x_m,), wait={f\"pipe-{x_m}\":stage_delay})\n",
    "            \n",
    "displayCanvas(canvas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# final result\n",
    "displayTensor(X)\n",
    "displayTensor(X_ref)\n",
    "X == X_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results\n",
    "\n",
    "Check the result by performing the dot product of `A` and `x` and ensuring that equals the `b` we started with. (The jth entry of `b` corresponds to the dot product of the jth _column_ of `A` and `x`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_check = Fiber(coords=range(N), initial=0)\n",
    "# displayTensor(B2)\n",
    "\n",
    "A_NM = A_MN.swapRanks()\n",
    "A2 = A_NM.getRoot()\n",
    "displayTensor(A2)\n",
    "\n",
    "# canvas2 = createCanvas(A, X, B2)\n",
    "for a2_n in range(N):\n",
    "    for a2_m in range(M):\n",
    "        # print(j, i, a_pay[i], x_val, b2_ref)\n",
    "        b2_ref = B_check.getPayloadRef(a2_n)\n",
    "        x_val = X.getPayload(a2_m)\n",
    "        b2_ref += A2.getPayload(a2_n, a2_m) * x_val \n",
    "        # addActivity(canvas2, (i,j), (i,), (i,))\n",
    "        \n",
    "# displayCanvas(canvas2)\n",
    "\n",
    "# check results\n",
    "B == B_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling schemes\n",
    "\n",
    "For your convenience, `A_MN` and `B_N` are repeated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayTensor(A_MN)\n",
    "displayTensor(B_N)\n",
    "\n",
    "numPEs = 2\n",
    "numLanes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A_MMNN (due to Joel)\n",
    "\n",
    "The program above must have `M` PEs for an MxM matrix. This is not tenable for large matrices, and also, such a pipeline would have load imbalance across the PEs: the first PE sees all elements of B, but the last sees just one.\n",
    "\n",
    "Tiling helps most immediately with the first problem, by splitting the problem into tiles, each of which can be solved by a system with a bounded number of PEs.\n",
    "\n",
    "Tiling is tantamount to creating an additional rank. Suppose we want to tile `A_MN` so that an `M0`-sized subset of rows can be processed by `M0` PEs at a time. We'll call `splitUniform(M0)` to produce a rank-3 tensor with ranks `M.1`, `M.0`, and `N`, called `A_MMN`. No fiber in rank `M.0` will contain more than `M0` payloads.\n",
    "\n",
    "If your ranks are all spatial (in other words, not temporal, as pipeline parallelism will do), then splitting one rank won't be enough. This is because `N` could possibly be unbounded, and each PE only has a finite number of elements (i.e., lanes) it can process at a time. This is why we also split the `N` rank with `splitUniform(N0)` so that each PE can process `N0` elements at a time in its `N0` lanes.\n",
    "\n",
    "This is how we produce a rank-4 tensor A_MMNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M0 = numPEs\n",
    "N0 = numLanes\n",
    "\n",
    "A_MMNN = A_MN.splitUniform(M0).splitUniform(N0, depth=2)\n",
    "displayTensor(A_MMNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A_MNMN\n",
    "\n",
    "The convention is to order ranks such that any ranks that are \"scale-up\" or unbounded appear closer to the root, and any bounded ranks further from them. This is called _normal form_.\n",
    "\n",
    "The jury is still out as to whether time-based ranks should appear above or below space-based ranks.\n",
    "\n",
    "To bring it to normal form, the `swapRanks()` at `depth=1` moves the bounded ranks to the bottom (conversely, unbounded ranks to the top)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_MNMN = A_MMNN.swapRanks(depth=1)\n",
    "displayTensor(A_MNMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A_MMMN (due to Neal)\n",
    "M0 in the number of lanes, M1 in the number of PEs, and all N elements streamed in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_MMMN = A_MN.splitUniform(numPEs).splitUniform(numLanes)\n",
    "# M.1.1 is \"M2\", which  and M.1.0 is \"M1\"\n",
    "A_MMMN.ranks[0].id = \"M.2\"\n",
    "A_MMMN.ranks[1].id = \"M.1\"\n",
    "displayTensor(A_MMMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A_MNMM\n",
    "To bring the unbounded rank (N) above bounded ranks (here, M1 for PEs, and M0 for lanes), need to swap N and M0 (depth 2), then N and M1 (depth 1).\n",
    "\n",
    "Note that this would use B_N, not B_NN (below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_MNMM = A_MMMN.swapRanks(depth=2).swapRanks(depth=1)\n",
    "displayTensor(A_MNMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_NN = B_N.splitUniform(N0)\n",
    "displayTensor(B_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X_MM (output for Joel's version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MM = Tensor(rank_ids=[\"M1\", \"M0\"])\n",
    "displayTensor(X_MM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fibertree Native Traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = Tensor(rank_ids=[\"M\", \"N\"], name = \"P\")\n",
    "X = Tensor(rank_ids=[\"M\"], name = \"X\")\n",
    "\n",
    "a_m = A_MN.getRoot()\n",
    "b_n = B_N.getRoot()\n",
    "p_m = P.getRoot()\n",
    "x_m = X.getRoot()\n",
    "\n",
    "# Initialize M[0] of P to be B, then don't use B anymore.\n",
    "p_n0 = P.getRoot().getPayloadRef(0)\n",
    "for n, (p_ref, b_val) in p_n0 << b_n:\n",
    "    p_ref <<= b_val\n",
    "\n",
    "canvas = createCanvas(A_MN, P, X)\n",
    "    \n",
    "for m, (x_ref, (p_n, a_n)) in x_m << (p_m << a_m):\n",
    "    p_n_prime = p_m.getPayloadRef(m+1)\n",
    "    is_first = True\n",
    "    prop_val = 0\n",
    "    for n, (p_ref, (p_val, a_val)) in p_n_prime << (p_n & a_n):\n",
    "        if is_first:\n",
    "            prop_val = p_val / a_val\n",
    "            x_ref <<= prop_val \n",
    "            is_first = False\n",
    "            addFrame(canvas, (m, n), (m, n), (m,))\n",
    "        else:\n",
    "            p_ref <<= p_val - prop_val * a_val\n",
    "            addFrame(canvas, (m, n), [(m, n), (m+1, n)], ())\n",
    "\n",
    "#displayTensor(A_MN)\n",
    "#displayTensor(P)\n",
    "#displayTensor(X)\n",
    "displayCanvas(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### MNMN traversal (thanks to Michael Pellauer) (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "P_MNMN = Tensor(rank_ids=[\"M1\", \"N1\", \"M0\", \"N0\"], name = \"P\")\n",
    "X_MM = Tensor(rank_ids=[\"M1\", \"M0\"], name = \"X\")\n",
    "\n",
    "a_m1 = A_MNMN.getRoot()\n",
    "b_n1 = B_NN.getRoot()\n",
    "p_m1 = P_MNMN.getRoot()\n",
    "x_m1 = X_MM.getRoot()\n",
    "\n",
    "# Initialize M1[0] of P to be B, then don't use B anymore.\n",
    "p_n1_0 = P_MNMN.getRoot().getPayloadRef(0)\n",
    "for n1, (p_m0, b_n0) in p_n1_0 << b_n1:\n",
    "    p_n0_0 = p_m0.getPayloadRef(0)\n",
    "    for n0, (p_ref, b_val) in p_n0_0 << b_n0:\n",
    "        p_ref <<= b_val\n",
    "\n",
    "#displayTensor(A_MNMN)\n",
    "#displayTensor(P_MNMN)\n",
    "\n",
    "canvas = createCanvas(A_MNMN, P_MNMN, X_MM)\n",
    "\n",
    "# note leading variable in for loop matches order of traversal\n",
    "# use << when it's an output\n",
    "for m1, (x_m0, (p_n1, a_n1)) in x_m1 << (p_m1 << a_m1):\n",
    "    is_first = [True for m0 in range(M0)]\n",
    "    prop_val = [0 for m0 in range(M0)]\n",
    "    for n1, (p_m0, a_m0) in p_n1 & a_n1:\n",
    "        for m0, (x_ref, (p_n0, a_n0)) in x_m0 << (p_m0 << a_m0):\n",
    "            m = m0\n",
    "            next_m = m+1\n",
    "            next_m1 = (next_m // M0) * M0\n",
    "            next_m0 = next_m\n",
    "            rel_m0 = m % M0\n",
    "            #print(f\"{m1}, {m0}\")\n",
    "            p_n0_prime = p_m1.getPayloadRef(next_m1, n1, next_m0)\n",
    "            for n0, (p_ref, (p_val, a_val)) in p_n0_prime << (p_n0 & a_n0):\n",
    "                if is_first[rel_m0]:\n",
    "                    prop_val[rel_m0] = p_val / a_val\n",
    "                    x_ref <<= prop_val[rel_m0]\n",
    "                    is_first[rel_m0] = False\n",
    "                    addFrame(canvas, (m1, n1, m0, n0), (m1, n1, m0, n0), (m1, m0))\n",
    "                else:\n",
    "                    p_ref <<= p_val - prop_val[rel_m0] * a_val\n",
    "                    addFrame(canvas, (m1, n1, m0, n0), [(m1, n1, m0, n0), (next_m1, n1, next_m0, n0)], ())\n",
    "#displayTensor(P_MNMN)\n",
    "displayCanvas(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X_MMM (output for Neal's version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MMM = Tensor(rank_ids=[\"M2\", \"M1\", \"M0\"])\n",
    "displayTensor(X_MMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
